{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a deep learning model on a large dataset\n",
    "\n",
    "- In Keras, using `fit()` and `predict()` is fine for smaller datasets which can be loaded into memory\n",
    "\n",
    "- But in practice, for most practical-use cases, almost all datasets are large and cannot be loaded into memory at once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(df, batch_size, path_tiles, num_classes):\n",
    "    \"\"\"This generator use a pandas DataFrame to read images (df.tile_name) from disk.\n",
    "    \"\"\"\n",
    "    N = df.shape[0]\n",
    "    while True:\n",
    "        for start in range(0, N, batch_size):\n",
    "            x_batch = []\n",
    "            y_batch = []\n",
    "            end = min(start + batch_size, N)\n",
    "            df_tmp = df[start:end]\n",
    "            ids_batch = df_tmp.tile_name\n",
    "            for id in ids_batch:\n",
    "                img = cv2.imread(path_tiles+'/{}'.format(id))\n",
    "                # [0] since duplicated names\n",
    "                labelname=df_tmp['y'][df_tmp.tile_name == id].values[0]  \n",
    "                labelname=np.asscalar(labelname)\n",
    "                x_batch.append(img)\n",
    "                y_batch.append(labelname)\n",
    "            x_batch = np.array(x_batch, np.float32) / 255\n",
    "            y_batch = utils.np_utils.to_categorical(y_batch, num_classes) \n",
    "            yield (x_batch, y_batch)\n",
    "\n",
    "model.fit_generator(generator=batch_generator(df_train, \n",
    "                                              batch_size=batch_size,\n",
    "                                              path_tiles=path_tiles,\n",
    "                                              num_classes=num_classes), \n",
    "                    steps_per_epoch=len(df_train) // batch_size, \n",
    "                    epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then instead of `model.fit()` we will use `model.fit_generator(generator=batch_generator(df_train, ...))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity:\n",
    "\n",
    "- Assume we have a csv file with two columns. The first column is the url link of images and the second column is the image label \n",
    "\n",
    "- Assume the csv can fit into memory -> The following `df = pd.read_csv(' ')` is doable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def data_gen(df, batch_size):\n",
    "    while True:\n",
    "        x_batch = np.zeros((batch_size, 3, 224, 224))\n",
    "        ## Assume we have two class (two lables)\n",
    "        y_batch = np.zeros((batch_size, 1))\n",
    "        for j in range(len(df['url']/batch_size)):\n",
    "            b = 0\n",
    "            for m, k in zip(df['url'].values[j*batch_size:(j+1)*batch_size], df['class'].values[j*batch_size:(j+1)*batch_size]):\n",
    "                x_batch[b] = m\n",
    "                y_batch[b] = k\n",
    "                b += 1\n",
    "            yield (x_batch, y_batch)\n",
    "\n",
    "\n",
    "model.fit_generator(generator=data_gen(df_train, batch_size=batch_size), steps_per_epoch=len(df_train) // batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Question: What if even that csv file can not fit into memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_chunk_train = pd.read_csv('*.csv', chunksize=10)\n",
    "\n",
    "C = 0\n",
    "for chunk in df_chunk_train:\n",
    "    C += 1\n",
    "\n",
    "def data_gen(df_chunk, batch_size):\n",
    "    while True:\n",
    "        for df in df_chunk:    \n",
    "            x_batch = np.zeros((batch_size, 3, 224, 224))\n",
    "            ## Assume we have two class (two lables)\n",
    "            y_batch = np.zeros((batch_size, 1))\n",
    "            for j in range(len(df['url']/batch_size)):\n",
    "                b = 0\n",
    "                for m, k in zip(df['url'].values[j*batch_size:(j+1)*batch_size], df['class'].values[j*batch_size:(j+1)*batch_size]):\n",
    "                    x_batch[b] = m\n",
    "                    y_batch[b] = k\n",
    "                    b += 1\n",
    "                yield (x_batch, y_batch)\n",
    "\n",
    "\n",
    "model.fit_generator(generator=data_gen(df_chunk_train, batch_size=batch_size), steps_per_epoch=10*C // batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "- One of the best ways to improve the performance of a Deep Learning model is to add more variations of data to the training set\n",
    "\n",
    "- want the dataset to be representative of the many different positions, angles, lightings, and miscellaneous distortions\n",
    "\n",
    "- In keras there are two ways:\n",
    "\n",
    "    - Use `ImageDataGenerator`\n",
    "    - Write our custom code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data/train',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'data/validation',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning (TL)\n",
    "\n",
    "- In practice a very few people train a Convolution network from scratch (random initialisation) because it is rare to get enough dataset. So, using pre-trained network weights as initialisations or a fixed feature extractor helps in solving most of the problems in hand\n",
    "\n",
    "- Very Deep Networks are expensive to train. The most complex models take weeks to train using hundreds of machines equipped with expensive GPUs\n",
    "\n",
    "- Determining the topology/flavour/training method/hyper parameters for deep learning is a black art with not much theory to guide you.\n",
    "\n",
    "- So, we need transfer learning \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "\n",
    "base_model = applications.vgg16.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    " \n",
    "i=0\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    i = i+1\n",
    "    print(i,layer.name)\n",
    "    \n",
    "\n",
    "x = base_model.output\n",
    "x = Dense(128, activation='sigmoid')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "\n",
    " \n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    " \n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.SGD(lr=0.001, momentum=0.9),metrics=[\"accuracy\"])\n",
    " \n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=100,\n",
    "        epochs=10,\n",
    "        callbacks = callbacks_list,\n",
    "        validation_data = validation_generator,\n",
    "        validation_steps=20\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
